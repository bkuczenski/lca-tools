==========
Fri Sep 29 15:18:39 -0700 2017

Back to the top.

What we want to happen is this:

User is noodling around on a thin client.
 * User requests a specific entity
   = CatalogQuery.get() -> catalog._dereference
     -> _entities? no
     -> _check_entity -> archive.retrieve_or_fetch_entity()
     	archive handles request.
	if archive is an Antelope remote, 
   




 * User performs a search;
   = index implementation... generator yields make_ref(p)
   = 

receives a list of entities:
   - with their signatures, but
   - without full dictionaries, and
   - without quantitative data
   = query 

==========
Fri Sep 22 12:24:56 -0700 2017

this should be moved into the repo.... 

Yesterday- "Finished" the catalog ref refactor but didn't test it.  Reflections after the fact were mainly focused on the increasingly urgent need to refactor providers according to the same interface spec.  Right now the Catalog doesn't distinguish between local and remote access (because remote access has not been implemented) but more or less ALL of the interface implementations (BasicImplementation subclasses) are dependent on a local archive.  

IT WASN'T SUPPOSED TO BE THIS WAY! Antelope is supposed to abstract the archive.

MOREOVER, the fully-featured catalog is too heavyweight, because the first thing I want prospective users to touch is a TERMINAL that performs only foreground computations locally.

SO:

 - We still need to write an Antelope provider that obtains info from web requests
 - We still need to write the Antelope SERVER that will answer requests via web api
and now we ALSO need to :
 - refactor Catalog into 
   * core functions
   * terminal functions
   * central repo functions
 - refactor the ancient provider interface:
   * abstract base class that is drawn from the same QueryInterface-- it is an interface provider, after all
   * local implementation -- creates a local archive
   * remote implementation 

Then we have a lingering problem: caching metadata about remote entities.  Queries like get_property, get_uuid, entity_type, and so on should ABSOLUTELY NOT incur a remote access after the entity is first retrieved.

We obviously got blocked up on entity_type quite a bit yesterday-- because current arch (which was maintained) has us asking the Catalog and not the Query.  Because the Query doesn't know.  but it should. 

And finally, we have the problem of flows and flow characterizations.  The Qdb, for all its incremental improvements, STILL relies on assigning CFs to the "actual" flow entities used in the computations.  Is this what we want, going forward?  Come to think of it, weren't we recently pondering the possibility of making this REQUIRED at the DetailedLciaResult level?  We shouldn't. 

Characterizations and Exchanges should be the meat of LciaResults-- those can both be constructed locally IN THE ABSENCE of any entities (e.g. using catalog refs instead of entities).  THAT is the model we should pursue.

Then the CatalogRefs need to know how to retrieve (meta)information about themselves.  

OK- the problem was the decision in the refactor to make the catalog refs anchored to a query, instead of the catalogs themselves.  What THAT means, especially with the nondeterministic selection of query implementations, is that "you never know what you're gonna get." which is SHITTY.

now, a Query has an origin and a catalog. and that's IT.  all the query does is ask the catalog for an interface.

OK things are starting to resolve a bit here.

the QUERY should know how to get_property, get_uuid, entity_type, and so on because the CATALOG knows it.  The CATALOG should cache all the entities it receives on a metadata level, and the query (with the fixed origin) should be confident in being able to retrieve just the one it wants (i.e. matching the Query's origin and the request's external_ref) because the CATALOG will have cached it in its entities list.

ALL the metadata queries should come down to a catalog._dereference call, so that it checks the local _entities dict first.

Presently, all catalog_ref instantiations go through _dereference because they call catalog.lookup()

So how would this work?  
 * You have a query, you make a request-- say for ALL PROCESSES.  
 * For this, you don't need to anchor the catalog refs.  You just get a bunch of unresolved catalog refs back-- (even though they AREN'T unresolved- they just aren't retrieved)
 * the Query gets an implementation-- the implementation is simply a passthrough to a web request, which spits back JSON
 * the implementation takes the passthrough stuff and makes it into a list of CatalogRefs
   = at this point we've been SENT all the JSON of ALL the processes.  
   = Maybe the server will not send e.g. tags and things for an unrestricted all-entities query.  So there's no need to cache it? but if you don't get tags, what do you get? just IDs? just name + comment? + spatial scope / compartment / signature fields + reference entity, kind of a lot of shit. it should kind of get cached.
   = let's keep going
 * you have a list of un-fetched (but RESOLVED to exist) catalog refs. you pick one at random and ask to look at its inventory.

HOLD UP.  The way that's supposed to work is, you go catalog_ref.inventory().  But that method only exists on ProcessRefs.  And you can only create a ProcessRef by giving it a Query.  And normatively (based on the way the code is written) the way you get a query is by FINDING an entity and making a query for the origin of the resource that actually HAS the entity.

In this case, we KNOW where the entities are coming rom because the query already asked for them.  and we know the entity type.

so we can just make a CatalogRef.from_query() because we are getting the RESPONSE from the origin KNOWN TO CONTAIN the processes.  And we've ALREADY done the costly part-- retrieving the list.

To a first order, one process METADATA entry is about 1k, so retrieving the full list of all processes in, say, ecoinvent, is 13000 x 1k  = 13 MB of data.  Even GZipped it will be 1-2MB.

In fact, ei3.2 undefined has 11552 processes and is 14,007,226 bytes; 2,196,017 gzipped

maybe we should disallow .processes() and .flows() for large databases (without a paying account :) )

ok, anyway.

let's say viewing all processes is disallowed via remote, and at least one kwarg is required to accept the request.  THEN say you search for 'steel' and get 574 results, gzipped to about 100k of bandwidth, in 2.5 sec.  SINCE you received that, and let's go ahead and say ALL the metadata were included, so you want to cache it.

at this point, it's the INTERFACE IMPLEMENTATION that has the results.  The Implementation can make catalog refs from the json, supplying a query (because it has _catalog and knows its own origin) and an entity type (because it was specified) and you wind up with a list of properly typed, anchored entity refs.  

what about reference entities? same again-- the external refs are supplied-- so you create flow refs (again, using the query) and create ref exchanges and install them

But then as soon as you want to do something with flows, say print them, you will need *their* reference entities-- but there are probably only like 20 of those-- and you can get them on an as-needed basis... or better... get local quantities from the Qdb.
NOW we are getting into some serious higher-order shits-fucked-up.  Because we just took out the provision that merged quantities from differing sources, for the express reason that we wanted different versions of LCIA methods to track differently.  (Looks like we can fix that just by asking the quantity if it is an lcia method, and then deciding to merge based on that) (DONE)

OOOOOOhKAAAAY.  anyway, we DO need to retrieve the quantities from remote, on an as-needed basis, and then subsequently add them to the Qdb (with merge in the case of non-LCIA-methods) for unit conversions to operate smoothly.  
The only way this is a problem is if iwe try to print a large set of flows, based on their catalog refs, but none of them have compartment information retrieved... at present that will generate a large number of separate get_property('Compartment') requests, which is terrible.

We don't want flow refs.  we want actual flows.

No, we want refs.  There is no way around it, the flows have to be queried from the remote source. The only advantage would be to querying them all at once- but again, you have a massive size issue.  Ecoinvent 3.2 undefined has 4680 flows totalling 2,877,168 bytes unzipped (253,306 bytes gzipped)

we do NOT want to ever run into the situation of having 1,800 distinct get_property() queries sent to a remote server.  

I think the Antelope server needs to send back a complete flow signature with every request that involves flows (minus characterizations) so that at least reference entities and compartments come through.

This means that exchanges and characterizations need to serialize differently for antelope requests. no prob.

then flow refs need to know how to query their own characterizations from remote.  and the antelope server needs to pass quantity requests through the Qdb to find the local match.

ok, this is helpful.

Back to our narrative.

 * You have a query, you make a request-- say for ALL PROCESSES having Name='steel'.
 * the Query gets an implementation-- the implementation is simply a passthrough to a web request, which spits back JSON
   = the JSON is a list of processes, with only reference exchanges, with the full (uncharacterized) flows
 * the implementation takes the passthrough stuff and makes it into a list of anchored, fully populated CatalogRefs
 * the implementation gives these back to the catalog to cache them
 * the implementation returns them 


Fri 2017-09-22 15:08:00 -0700

It is really clarifying (and agonizing) to think this through in terms of what functionality we actually want on a thin client.  e.g. charts? (if so, we need matplotib) background computation? (scipy) and so on.



It's really all about the INTERFACE (as specified in iquery.py).  This SHOULD tell us everything we would ever want to know (need to add: characterizations for flows; terminations for fragments (?) )
NO: terminations are not included.  you get traversal results for fragments.  you do need to know supported scenarios. 

Let's audit the use of _archive in the Implementations-- and then call it a day.

60 usages.  Most of them are retrieve_or_fetch_entity-- so the question is when does the retrieved entity need to be the literal entity and when can it be a remote ref?


Basic: 9 usages
 * __getitem__: return self._archive[item] --> not valid for remote
#* __str__: needs _archive.source --> should be known from resource
 * characterize(): abandoning
#* fetch: -> retrieve_or_fetch_entity -> catalog ref OK!
#* get: -> same as fetch
#* get_item: -> catalog ref OK!
#* get_reference: catalog ref OK!
#* get_uuid: -> catalog ref OK!
#* origin: _archive.ref --> should be known from resource

Index: 7 usages
#* .flows(**kwarg) --> need implementation
#* .processes(**kwarg) ""
#* .quantities(**kwarg) ""
 * .get -> useless; already in basic
#* .terminate() --> need implementation
#* .originate() ""
 * .mix --> jury's still out

Inventory: 6 usages

Short version is, no, this is going to need to be completely rewritten (and background too)
maybe quantity provides the means how: simply allow the _archive to override the implementation.
I think that's the winning approach.
and then the implementation provides the default mechanism for local archives.

The last question, then, is whether we want to make the Catalog the default cache for ALL archives and not just remote ones? i.e. change BasicImplementation.  I think not.

---crufty---

Background: this is the hardest one. do it last.
 * if static (web version is no) then we install a local background manager
   since it's not static, we install the proxy-- 
   but it's not local- so we don't want a fg proxy, we want a web proxy that will actually pass
	    


The implementation hands those BACK to the 

After that, ALL the non-metadata queries should come down to implementation. Then the IMPLEMENTATION is either:

 * (for REMOTE providers) ask the antelope server and take the result.  
   RO: Strings-- nothing to cache, nothing to do. Just give it to the user. (only for Quantity iface)
   R1: entities-- just straight json, except use it to make a CatalogRef




==========
Tue Sep 19 15:08:01 -0700 2017

Progress?  We made the decision to refactor ForegroundCatalog but we have yet to actually do it.

We DO have a test case, however: to successfully import the AUOMA model.  So let's start there.

No, let's first start by refactoring foreground_catalog, and then using AUOMA as a test case from there.

first mistake of the day: should have merged WaterfallCharts into master instead of into ReCatalog.

oh well live and learn.  ReCatalog will be merged soon enough.

So what do we want to accomplish with Foreground Catalog?

 * if fragments are just another entity type, why do we need another catalog type? ans: we don't.
   - previously, we utilized the difference when de-serializing FlowTerminations.  If they terminate to processes, we created a catalog_ref; otherwise, we just retrieved the fragment itself.
   - but now, we want that catalog ref to stand in for fragment, just as it stands in for process.
   - all we need to do is provide foregrounds as a new LcResource.
     = we can do that with existing code using catalog.new_resource()
     = only we can't EXACTLY, because an LcForeground needs a catalog as an argument.  We can provide it- but we should do so automatically.
     == Ah! catalog is already provided to create_archive() because it's needed in archive_from_json-- so we just need to add another case (done)
   - so wtf is the ForegroundCatalog supposed to do?? we identified 4 features:
     ** ed: hi, I'm ed
     ** __getitem__(self, name) -> retrieves named foreground.  We DID want foregrounds to be scoped distinctly.
     ** frag()
     ** fragment_lcia()

The editor is [only, presently] used in the LcForeground. So why don't we put the fucking editor straight in the fucking foreground?
 1. because the editor needs a Qdb
 2. because we only want to have one editor. (??)

So why don't we just put the fucking editor in the LcCatalog?
 3. because we want to disallow fragment creation for simple catalogs? seems like an odd / quixotic decision.
 4. because LcCatalog is already too heavy? just like catalog_ref?

Getitem could be useful for catalogs in general-- presently it pulls the archive by name.  could just transplant that.

Frag() -- aha, now this is tricky.  We need a way of spooling through the existing, LOADED foregrounds and looking for a fragment by ref or leading uuid portion.-  That's as simple as filtering by type

Fragment LCIA. This gets to a more fundamental question.
Fragment LCIA operates on a list of FragmentFlows (and relies on flow terminations caching nested fragment flows internally)
 - it needs qdb
 - it needs to tell qdb to load factors
 - it needs to call itself recursively.

So... how are we going to be invoking this in the REAL world?  if we are going to be calling it as a catalog_ref method, then we want to model the behavior of the entity itself.
EXCEPT-- a fragment ENTITY does not know how to compute its own LCIA-- it needs a catalog for that.
Is there a precedent? YES- catalog_refs have all sorts of methods (mainly background methods) that don't work on literal entities.

So- IF we made Fragment LCIA a catalog_ref function-- then what would it do, and where would it be implemented?
 ans: it would be implemented in the inventory interface... the InventoryImplementation inherits the BasicImplementation, which keeps a pointer to the catalog, which has a Qdb
 Better- the BasicImplementation is also one of n places where we're already keeping a log of characterized quantities-- but that's a different list-- because that's a list of quantities that are looked up externally and applied to native flows in the archive
 so hmm.

NO NO NO. The Qdb ALONE should keep track of what quantities have been loaded into the Qdb.  Right now that is not done because the Qdb only has add_cf, not add_all_cfs_for_quantity.

I'm seriously recursing here....

where are we at?

AUOMA Testing
-> ForegroundCatalog refactor
  -> fragment_lcia()
    -> catalog_ref()
      -> Catalog.load_lcia_factors()
        -> Qdb.get_canonical_quantity()

What on earth do we want a canonical quantity for?  ans: the canonical quantity is the actual entity stored in the Qdb.  We want to retrieve it if we want to know whether it's loaded?  BUT- the literal (canonical) quantity ITSELF can't tell me its factors, because that is a catalog function.

Where is get_canonical_quantity used? 6 places
 * LcCatalog.lcia() -> to test if loaded; as argument to do_lcia
 * LcCatalog.load_lcia_factors() -> to store a list of loaded quantities
 * Qdb.do_lcia() -> to dance around a bit before retrieving the same quantity with get_quantity() (which is functionally identical)
 * in BackgroundImplementation.bg_lcia(), in an if False: block (linked with BasicImplementation.characterize()
 * in QuantityImplementation.lcia_methods(), in an apparent NOP (return value not used)
 * in QuantityImplementation.quantities(), in the same apparent NOP (return value not used)
   = now it's not really a NOP because it calls _get_q_ind, which ADDS the quantity if it doesn't already exist. But isn't there a better way of doing that?

More to the point- the original question still stands: Why do we ever WANT to obtain a "canonical" quantity?  What does it allow us to do that having a quantity_ref.uuid doesn't?



==========
Tue Aug 01 13:25:08 -0700 2017

This is still stalled out.

We're having a breakdown of design discipline.

Tue 2017-08-01 15:14:48 -0700

Big improvement (not yet tested though): re-designed QueryInterface to inherit from a whole bunch of abstract base classes that each only specifies how different resource types should behave. Then instead of re-inheriting from the QueryInterface, I now have a separate set of implementation classes that actually implement the endpoints.

NOTHING is tested or even compiled yet.  but it already feels better.

Next tasks: get stuff out of catalog.py, and then get more stuff out of foreground_catalog.py

Tue 2017-08-01 22:59:37 -0700

OK- LcCatalog actually looks pretty good. We have:
 - a whole list of imports, many of them local

Line 55: class LcCatalog(object):
then we have 60 lines of root-path-dependent constants

line 118: __init__
     _rootdir
     _resolver
     _entities ** DO WE REALLY WANT THIS? I THINK WE SHOULD GET RID OF THIS **
     _qdb

The actual catalog:

     _archives: source to LcArchive
     _names: ref:interface to source
     _nicknames: shorthand -> source

     _lcia_methods: ** WHERE SHOULD I KEEP TRACK OF THIS? **  (already 2+ places: in catalog, in qdb, in basic)

Lines 147-230: make, get, retrieve archives and resources

Lines 245-287: names, sources, and interfaces

lines 288-381: index, cache, and archive resources

lines 382-418: dereference and retrieve entities

lines 419-447: get interfaces

lines 448-472: Utilities
	query(origin) - returns a CatalogQuery
	lookup(ref) - ref = CatalogRef - returns "the lowest-priority origin to contain the entity". sounds like it could have greater utility.
	fetch(ref) - this should also permit greater specification
	entity_type(ref) - could probably be done more efficiently

lines 473-eof (538): Qdb Interaction
	is_elementary(flow)
	is_loaded(lcia) ** THIS AGAIN **
	load_lcia_factors(ref)
	lcia(p_ref, q_ref) -- the core function of the catalog
	annotate(flow, quantity, ...) -- gets stored in local Qdb
	quantify(xxx) - not helpful
	   

Glue / misc:
line 144-146: quantities()
line 232-235: privacy(ref)
line 236-244: flowables grid

All seem fair.

(commit)

Now, turning to foreground_catalog...

WHY, exactly, are we creating an entire separate (and ad hoc) infrastructure for handling "foregrounds", which in fact correspond to physical sources in the EXACT same way that LcResources do, and we ALREADY HAVE a catalog for handling LcResources?

That is a very good question. I think basically all that stuff can COME OUT, modulo some light details regarding search order / preference / priority.

line 18-26: class ForegroundCatalog(LcCatalog): and docstring
lines 27-34: a list of foregrounds known outside the LcResolver context
lines 35-45: we want sources to exclude foregrounds for some reason? so we ARE storing foregrounds in the same dict as other sources, BUT monkeying with the machinery to except them?

line 46: __init__
     _foregrounds -> only loaded ones (so, unlike _archives how?)
     _known_fgs -> redundant to resolver
**     ed -> Hi, I'm ed.

** lines 53-65: __getitem__ this is maybe appealing
lines 66-78: this terrible name-canonicalizing. Why am I forcing names to start with 'foreground' and have no specified interface, when the existing machinery already provides ref:foreground?
ans: because I want to be able to specify a scope of all foregrounds? well, just make the resolver work with '' input

lines 79-109: load / add foregrounds
lines 110-118: known foregrounds management. again, this is a matter that LcResources already solve
lines 119-127: listing + showing foregrounds

Then we have 128-155: re-implement _dereference as _retrieve for no good reason
** lines 156-164: the first foreground-specific feature, other than __getitem__ but that is also good: get fragment by reference or leading portion of uuid

lines 165-184: Re-implemented lookup() and fetch()

** lines 185-217 fragment_lcia(fragmentflows, q_ref)

OK. so about 32 + 8 + 12 + 1 = 25% of the class is worth keeping.  The rest should be wrapped up in LcCatalog.

Wed 2017-08-02 15:44:19 -0700

So how do we go about this?

 * Do we want 'foreground' to be included in INTERFACE_TYPES?
   - are there any characteristics of the foreground interface that are not shared by any of the others?
   - are there compelling circumstances when I would NOT want to look in foreground resources?
   = does the order of inquiry matter, or is that handled with resource priorities?

INTERFACE_TYPES uses (8):
 iquery:
  (as arguments to _perform_query)
  - get_item
  - get
  - get_reference
  - get_uuid
  (as default interfaces to resolve)
  - resolve

 test_catalog:
  - initialize uslci_bg resource

 catalog:
  - lookup(), as argument to _dereference

 lc_resource:
  - to validate interfaces argument

So add_foreground should create a resource that has 'foreground', 'index', 'inventory', and 'foreground' should be in INTERFACE_TYPES.

the only thing foregrounds can do that others can't is be WRITEABLE.  So foreground interface is all POST routines.  everything else either belongs to the catalog (fragment_lcia) or .. is traverse() and can go in inventory.  there aren't any others.

also need a scenarios interface.

(the interfaces lark is quite a gas, isn't it?)

so foreground is all about creating a foreground, creating flows, setting properties, creating fragments and specifying exchange values, and terminating. and setting balance
that's it.
scenarios-> those are just query args for exch val and termination

Thus we add 'foreground' to INTERFACE_TYPES, and we create READONLY_INTERFACE_TYPES which excludes it.

Then almost all of the above cases (6 of 8) should in fact use READONLY; and only resolve() and lc_resource validation should use the full list.

 * What other changes need to be made in order to have the LcResolver handle foregrounds?

 * What queries need to be supported in the foreground query interface?
   Foreground interface is the same as inventory interface, except that it allows for the archive to contain fragments.
   = is there an affirmative characteristic of LcArchive that PREVENTS it from storing fragments? ANS: NO

Foreground audit!! Now this was recently rewritten, so I imagine it is generally useful, but let's take a look:

line 22-26: class LcForeground(LcArchive): + docstring
line 27-30: load_json_file-- this could be part of the base LcArchive; I don't see why not
line 31-46: external references mapping-- this could be part of the base LcArchive
line 47-63: __init__(path, catalog)
     Requires a catalog to deserialize fragments
     Requires that it be a foreground catalog i.e. that has an entity editor
line 64-67: editor property
line 68-77: create resolved catalog refs; also actual entities from foregrounds (DEPRECATED)
line 78-90: override add to merge instead of skipping
line 91-96: save-- write entity file, serialize fragments
line 97-167: create fragments
line 168-172: clear score caches
line 173-180: path-specific constants
line 181-240: access / retrieve fragments
line 241-245: draw tree
line 246-250: override check_counter
line 251-259: assign external references
line 260-293: load fragments
line 294-304: outmoded child_flows fn
line 305-325: save fragments
line 326-360: find linked terms and orphans.

Things to fix:
 - old child flows
 - reorganize
 - remove fragment handling from LcArchive.add(), put it into Foreground.add(). This prevents LcArchives from adding fragments.  raise (and catch?) TypeError.
 - Terminations to fragments shjould always reference the parent fragment, with the termflow used to indicate if the fragment is not being run directly.

 - figure out the upstreaming issue.  Is this just a foreground feature now? should it always and only use the catalog's native Qdb?

We need to think this over more.






==========
Tue May 02 14:53:31 -0700 2017

This is STALLED OUT.

Here's the approach:

 The CatalogRef is the magic key.  It implements all the API interfaces, and then it asks the catalog to satisfy them.
 The Catalog leans heavily on the resolver to get from a semantic ref to a physical archive.
 


==========
Tue Apr 25 15:07:52 -0700 2017

An LcCatalog provides an automated service for obtaining information about LCA data objects.

There are four different forms of (read-only) information available to be queried (see API.md):

 * Quantity data are informations about the quantitative properties of flows.  These are stored in a quantity database, which can exist independently of any inventory information.  Quantity data apply to 'flowables', often when going into or coming out of different 'compartments'.  An LcFlow is a flowable and a particular compartment.

 * Catalog data are metadata and non-quantitative data about entities, including their names, characteristics. At least a subset of these are generally publicly available for all entities.

 * Foreground data are quantitative data that describe computational information about the entities when used in models.  For flows, these include characterizations. For processes, these include exchanges.

 * Background data are quantitative data that describe the aggregated life cycle inventory or life cycle impact assessment results for processes embedded in computable databases.

When a catalog ref is looked up, the catalog specifies whether the entity is known to the catalog in a catalog, fg, or bg sense.

This means that the catalog is also separate from the foreground in a way that's different from before. it's more focused on data retrieval (as should be).

We still haven't broached the study construction- which comes next.

Thu 2017-04-27 12:17:52 -0700

But-- for the time being, the important facts are: the catalog PROVIDES access to the API routes specified in API.md.  The Catalog CAN be implemented as a web service.  The catalog CAN ALSO operate as a web CLIENT- in which case the data source would be another instance of the catalog service-- but this is a complicated step that requires the catalog to de-serialize JSON data.  That can get folded into the archive factory, though, I think.

SO, TO SUMMARIZE:
 - the LcCatalog is the culmination of the semantic catalog work. the idea is to translate a REST-style query into usable lca data. I *think* the API spec is adequate to do complete LCA. (minus uncertainty; minus study-specific allocation).

 - The catalog operates on a catalog ref, which is an origin + external ref, which should be unique.

 - the STUDY should use the catalog to obtain data and also enable the user to directly input data. Everything is fragments; nothing is processes.

 - fragments are basically exchanges, and terminations are complementary exchanges.  Thus a terminated fragment is a link.

REMAINING WORK:

 - implement the catalog, obviously

 - implement the CatalogInterface, ForegroundInterface, BackgroundInterface.  get them tested and running.  Those interfaces are eminently testable.

 - re-design the foreground to be the study-- construct fragments incrementally using the API queries.

 - for now we don't need fg lcia, since all the child flows will be rendered as fragments.  If we want, we can do something where 'flows not present as child fragments but present in the process inventory can be characterized'.  but rather than jump to that, we should AUTOMATICALLY CREATE SUB-FRAGMENTS from processes, store the process ref, and then write a routine to update fragment exchange values from queries.  best of all worlds.

 - lastly, write the web service, and get it running on the lca-tools-datafiles data.  THAT will be instantly disruptive.

time to get the bus.

PLUMBING

OK, in the interest of postponing ACTUAL PROGRAMMING for as long as possible, let's continue to think through exactly how this will work.

Fri 2017-04-28 18:23:14 -0700

OK.  SOME modest progress.




Option 1: Prototype the CatalogInterface, ForegroundInterface, and BackgroundInterface.

 * CatalogInterface inherits from NsUuidArchive
 * key is '/'.join(origin, external_ref)
 * Catalog's entity list acts as a cache -- stores non-quantitative data for all entities requested (quant too?)
 * CatalogInterface implements the API
 * CatalogInterface composed with a Qdb (separate interface)
 * CatalogInterface maintains a collection of LcArchives

 * CatalogInterface has a resolver input file that maps semantic roots to lists of data sources
   - each data source has attributes:
     = Priority - lowest priority accessed first
     = dataSourceType - argument to archive_factory
     = dataContents - controlled vocabulary: 'catalog', 'foreground', 'background'
       ** foreground provides catalog
       ** foreground archive input to BackgroundManager provides background
       ** background provides catalog
       ** background cannot provide foreground
     = ExcludeRoutes: list of API routes that the source should not provide
     = AccessControl: controlled vocabulary:
       ** 'open' -- any query is answered
       ** 'metered' -- authentication must be provided; every query is billable
       ** 'protected' -- authentication must be provided; authorization TBD
       ** ... others TBD

There is going to have to be some kind of regression.  When a query is received, the resolver needs to:
 - receive the query, including origin + ref + authentication info
 - determine whether the catalog can answer the query at all
   = determine if a source is known for the given origin
   = determine if the request is authorized to access the source
     ** list the same source multiple times?? sure-- the thing that gets 
   = determine if the query type (catalog, fg, bg) is available
 - determin
