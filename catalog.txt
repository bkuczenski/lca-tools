==========
Mon Oct 30 10:09:46 -0700 2017

Debugging the work... I've come across a problematic situation which is the FlowTermination.is_subfrag property.

A subfrag is a fragment termination that is not foreground and not background.

A background termination is one whose parent is marked background OR whose termination is marked background.

This becomes a problem if the termination is from a different foreground, because the interface doesn't currently support querying is_background.

So there are three options:
 * extend the interface to include is_background for FragmentRefs
 * refactor the is_bg test to be more correct
 * refactor the is_subfrag test to be more correct

since the first option is undesirable unless there's a true requirement, we want to know WHAT is is_subfrag used for and what is is_bg used for?

first- what is is_background even used for?

ans:
 1. If the FRAGMENT is marked background, then it ends traversal
 2. The fragment string representation is marked (B)
 3. LcForeground optionally selects foreground vs background fragments
 4. fragment.cutoffs() extends cutoffs with those found in background fragments (**DWR**)
 5. if TERM NODE is background, then [relying on 1 above] its traversal result replaces the current fragment
 6. if TERM NODE is background, the termination serializes as --C or -B
 7. if term is subfrag- raise SubFragmentAggregation on traversal
    else- if parent is background- perform bg_lcia
    (implicit: if parent is not background but term_node is background, then is_subfrag will be false but we'll end up treating the term node as a fg process and feed _unobserved_exchanges() to qdb. this seems broken.)

OK, so there's a bit of a mess

FYI, the antelope query is_background should be replaced with is_in_background to distinguish semantically- because it is asking the partial ordering whether the process is in the background, rather than if it "is" a background process.

Now then- what about is_subfrag? I have a feeling that is mostly bound up in #7 above, but let's see:
 1. used for spacing in the TeX report-- to detect whether to widen the node position
 2. if fragment's parent's termination is_subfrag, then fragment is not observable-- its value is set by traversal
 3. if a FragmentFlow termination is_subfrag, and the termination descend is False, then subfrags need to be retained
 4. in fragment creation, again to determine observability
 5. ** in FlowTermination.set_term_params(): flip directionality depending on whether we are connecting to a subfragment's reference flow or IO flow-- again, this seems broken to be excluding cases where the term_node is background
 6. #7 above- in compute_unit_scores

and that's it.

Mon 2017-10-30 11:00:07 -0700

LAST thing to check is the usage of Termination.is_bg():

 1. for sorting child nodes in TeX report
 2. for adjusting spacing-- to make smaller
 3. ** if term is_bg then we terminate it right there instead of making it a subfrag ** this behavior will change!!
 4. in show_tree, again for sorting
 5. in string formatting the termination


So, to summarize:
The reason we have been excluding background terminations from being considered as subfragments appears to be mainly drawn from convenience, to reduce the complexity of accounting for traversals with dedicated (UO-LCA-style) background fragments- we've been wanting to auto-merge two columns of Af instead of simply distinguishing between Af and Ad.  As noted above in two places, this seems to break expectations.  solution is probably to do away with this behavior.

HOWEVER, there also seems to be a need to distinguish local from remote subfragments.  There's a strong argument that a remote fragment should be treated just like a process, should be required to behave the same as a process.  That will solve the problem of lacking local access to traversal results because there is no expectation.

So what that means is a remotely-terminated node is NOT a subfrag.  even if it's a fragment.  Since we can put a process anywhere, this shouldn't affect anything in the traversal.  The unit scores should be computed the same either way.


==========
Thu Oct 26 12:54:15 -0700 2017

OK dammit. This needs to move forward. I was *mostly* done refactoring LcCatalog; all that remained was the fetch + dereference machinery.  In the spirit of simplicity, I'm going to say that the catalog should not provide a cache.  What it should do is provide catalog references.

The problem is, I still haven't figured out how it's supposed to work.  And what is the fucking point of all the interfaces?

The point is for both the query and the implementation to implement the same interfaces. But maybe that whole concept is wrong anyway.  CatalogRef certainly doesn't implement them.


==========
Tue Oct 24 12:26:09 -0700 2017

3 weeks later... and we're left with an incomplete refactor in LcCatalog.  It's NEARLY complete- there are error marks by everything that must be fixed.  But we have lost track of what we are doing.

So- let's review.

The Catalog-- which runs in a thin container-- keeps a cache of all the entities it queries.  That cache includes CatalogRefs, except flow refs need to be actual flows so as to have characterization abilities (?)

Tue 2017-10-24 12:42:47 -0700

Honestly, I can't remember where I am at with all of this.  The whole thing is a mess; the LcEntity structure is kind of fatally flawed.  The reliance on UUIDs everywhere, when UUIDs are suddenly out of vogue- external refs are also supposed to be unique.  UUIDs should be totally invisible to the user EXCEPT when no external ref is explicitly specified.  Entity UUIDs should definitely not be serialized: only external refs should show up in serializations.

Why does the query interface need get_uuid()? because uuids are used everywhere to uniquely identify entities. but I'm saying entity.link should be used to uniquely identify an entity.

fuck all.

Let's think this through one more time.

User instantiates an LcCatalog in a container that knows nothing about anything, except it has a set of resource files.
User can submit queries to any of the resources.  If the queries result in responses that are entities, the catalog caches them as entity refs.  [flow refs ARE entity refs that happen to be entities; as opposed to process refs which are NOT entities; quantity refs should get stored in the local Qdb which is used to dereference ALL quantities.]

Anytime the user asks


so to recap-- user submits a query to a specified origin.  the CatalogQuery determines what interface is being requested. The LcCatalog 

generates LcResource objects that implement the given interface for the specified origin.  The CatalogQuery should then ask the resource to answer the query, and return the successful result as soon as one is found.

IF the query response is an entity



==========
Mon Oct 02 06:27:11 -0700 2017

Here's the problem: the catalog does too much.
I've known this for a long time but only now figured out how to fix it.

the LcResources are the place to store archives.  Much of the mess surrounding indexing+archiving (e.g. _find_single_source) is because of failure to realize this and goes away if the resource actually *provides* the resource.  Ditto _get_interfaces -> gets moved into the resource, which already knows what kinds of interfaces it provides.

The resource is responsible for retrieving entity information and delivering either catalog refs or query responses. the resource is responsible for implementing the interfaces.

so the way this works is:
 * user client accesses catalog, gets list of semantic refs
 * picks a ref and asks a query
   - the catalog resolves the query to a set of resources
   - asks them sequentially (according to priority) to answer the query
   - each resource, to its ability, answers the query by implementing the interface against its archive
   - returns 2-tuple: entity_refs, result (or None if the entity refs are the result)
   - the catalog logs the entity refs in its local cache
     = which gets serialized using only local content so as to avoid stale remote data issues
   - the catalog delivers the result to the client

 * catalog can also make indexes + caches for local resources-> which implies making new resources

Let's review required changes.

catalog loses _archives
-> _names. What is this for?
   : for @property names (not used);
   : in get_archive --> which is now done by the resolver-> get resource -> archive
     :: this is private anyway since we don't want the user to circumvent the interface structure in general practice
   : for "hunting for" archives
   => looks obsolete.  pull them both.
   => ok, so _register_archive is entirely obsolete
-> _is_loaded: now just asks the resource
-> load_all
   : moved to resource, if static
-> add_nickname
   : never had occasion to use this yet (would help for pata/virent upgrade)
   : nickname was to SOURCE; really a nickname is for a resource (which has but one source)
   => so we need a new resolver method to find_resource_with_source
      - um, resources_with_source() already exists
   => and catalog._nicknames behaves the same
-> @property sources  also unused; also could be offloaded to the resolver
-> _index_source()
   : _register_index
   : index_resource
   : create_source_cache
   : create_static_archive
   => needs refactored.  but I don't think this blocks anything
   * let's be specific:
     - _find_single_source actually doesn't change; does count checking so we should keep it
       except return the LcResource rather than the source itself
     - _index_source moves into LcResource; catalog calls it and provides the inx file
     index_resource: finds source; indexes it; registers it. um, very little refactor needed
     - create_source_cache:

     Almost none of this changes. The only complexity is in how to uniquely specify resources (which honestly REMAINS a complexity, but is not produced nor solved)

     OK. let's try it; notes go here.

-> _retrieve_entity
   : _dereference
   : _get_interfaces
   => all get refactored as per above.

That's a clean review! do it.


Mon 2017-10-02 11:02:29 -0700

I have concerns.  Isn't it possible for multiple resources to have the same source file? and if so, doesn't that mean we end up with duplicate archive instantiations??  and isn't that the reason why we designed the catalog the way we did???

Moreover, and more fundamentally, create_archive needs a catalog (for foregrounds and for JSON upstream references, which are already kind of sketchy)
but even if we did ditch the upstream reference handling, which I would like to pull out frankly, it still leaves us requiring catalog to instantiate a foreground.
but that's fine- the catalog will be instructing the LcResource to instantiate; so just pass itself as an argument.

but the multiple-resources-same-source flaw:

PRACTICALLY, is this even a good idea?  I can imagine one resource having multiple sources, which is why we have the multiple interfaces and why everything is a generator.  but multiple resources being provided by the same archive? doesn't. really. make. sense.

IF we are not going to allow it, we should prohibit it.  That just means checking len([r for r in resolver.resources_with_source(s)]) == 0 before instantiating a new archive.

the fuzzy query already allows us to answer lower specificity queries with higher specificity resources (with priority).  I can't really imagine a good reason (other than some weird compatibility thing that I would probably want to discourage anyway) to allow the same archive to include multiple resources.  maybe the same antelope server, but in that case there is no cost to multiple archive instantiations since they don't store anything.  Though it does argue against prohibiting duplicate sources.

OK. onward. fuckin a.


==========
Fri Sep 29 15:18:39 -0700 2017

Back to the top.

What we want to happen is this:

User is noodling around on a thin client.
 * User requests a specific entity
   = CatalogQuery.get() -> catalog._dereference
     -> _entities? no
     -> _check_entity -> archive.retrieve_or_fetch_entity()
     	archive handles request.
	if archive is an Antelope remote, 
   




 * User performs a search;
   = index implementation... generator yields make_ref(p)
   = 

receives a list of entities:
   - with their signatures, but
   - without full dictionaries, and
   - without quantitative data
   = query 

==========
Fri Sep 22 12:24:56 -0700 2017

this should be moved into the repo.... 

Yesterday- "Finished" the catalog ref refactor but didn't test it.  Reflections after the fact were mainly focused on the increasingly urgent need to refactor providers according to the same interface spec.  Right now the Catalog doesn't distinguish between local and remote access (because remote access has not been implemented) but more or less ALL of the interface implementations (BasicImplementation subclasses) are dependent on a local archive.  

IT WASN'T SUPPOSED TO BE THIS WAY! Antelope is supposed to abstract the archive.

MOREOVER, the fully-featured catalog is too heavyweight, because the first thing I want prospective users to touch is a TERMINAL that performs only foreground computations locally.

SO:

 - We still need to write an Antelope provider that obtains info from web requests
 - We still need to write the Antelope SERVER that will answer requests via web api
and now we ALSO need to :
 - refactor Catalog into 
   * core functions
   * terminal functions
   * central repo functions
 - refactor the ancient provider interface:
   * abstract base class that is drawn from the same QueryInterface-- it is an interface provider, after all
   * local implementation -- creates a local archive
   * remote implementation 

Then we have a lingering problem: caching metadata about remote entities.  Queries like get_property, get_uuid, entity_type, and so on should ABSOLUTELY NOT incur a remote access after the entity is first retrieved.

We obviously got blocked up on entity_type quite a bit yesterday-- because current arch (which was maintained) has us asking the Catalog and not the Query.  Because the Query doesn't know.  but it should. 

And finally, we have the problem of flows and flow characterizations.  The Qdb, for all its incremental improvements, STILL relies on assigning CFs to the "actual" flow entities used in the computations.  Is this what we want, going forward?  Come to think of it, weren't we recently pondering the possibility of making this REQUIRED at the DetailedLciaResult level?  We shouldn't. 

Characterizations and Exchanges should be the meat of LciaResults-- those can both be constructed locally IN THE ABSENCE of any entities (e.g. using catalog refs instead of entities).  THAT is the model we should pursue.

Then the CatalogRefs need to know how to retrieve (meta)information about themselves.  

OK- the problem was the decision in the refactor to make the catalog refs anchored to a query, instead of the catalogs themselves.  What THAT means, especially with the nondeterministic selection of query implementations, is that "you never know what you're gonna get." which is SHITTY.

now, a Query has an origin and a catalog. and that's IT.  all the query does is ask the catalog for an interface.

OK things are starting to resolve a bit here.

the QUERY should know how to get_property, get_uuid, entity_type, and so on because the CATALOG knows it.  The CATALOG should cache all the entities it receives on a metadata level, and the query (with the fixed origin) should be confident in being able to retrieve just the one it wants (i.e. matching the Query's origin and the request's external_ref) because the CATALOG will have cached it in its entities list.

ALL the metadata queries should come down to a catalog._dereference call, so that it checks the local _entities dict first.

Presently, all catalog_ref instantiations go through _dereference because they call catalog.lookup()

So how would this work?  
 * You have a query, you make a request-- say for ALL PROCESSES.  
 * For this, you don't need to anchor the catalog refs.  You just get a bunch of unresolved catalog refs back-- (even though they AREN'T unresolved- they just aren't retrieved)
 * the Query gets an implementation-- the implementation is simply a passthrough to a web request, which spits back JSON
 * the implementation takes the passthrough stuff and makes it into a list of CatalogRefs
   = at this point we've been SENT all the JSON of ALL the processes.  
   = Maybe the server will not send e.g. tags and things for an unrestricted all-entities query.  So there's no need to cache it? but if you don't get tags, what do you get? just IDs? just name + comment? + spatial scope / compartment / signature fields + reference entity, kind of a lot of shit. it should kind of get cached.
   = let's keep going
 * you have a list of un-fetched (but RESOLVED to exist) catalog refs. you pick one at random and ask to look at its inventory.

HOLD UP.  The way that's supposed to work is, you go catalog_ref.inventory().  But that method only exists on ProcessRefs.  And you can only create a ProcessRef by giving it a Query.  And normatively (based on the way the code is written) the way you get a query is by FINDING an entity and making a query for the origin of the resource that actually HAS the entity.

In this case, we KNOW where the entities are coming rom because the query already asked for them.  and we know the entity type.

so we can just make a CatalogRef.from_query() because we are getting the RESPONSE from the origin KNOWN TO CONTAIN the processes.  And we've ALREADY done the costly part-- retrieving the list.

To a first order, one process METADATA entry is about 1k, so retrieving the full list of all processes in, say, ecoinvent, is 13000 x 1k  = 13 MB of data.  Even GZipped it will be 1-2MB.

In fact, ei3.2 undefined has 11552 processes and is 14,007,226 bytes; 2,196,017 gzipped

maybe we should disallow .processes() and .flows() for large databases (without a paying account :) )

ok, anyway.

let's say viewing all processes is disallowed via remote, and at least one kwarg is required to accept the request.  THEN say you search for 'steel' and get 574 results, gzipped to about 100k of bandwidth, in 2.5 sec.  SINCE you received that, and let's go ahead and say ALL the metadata were included, so you want to cache it.

at this point, it's the INTERFACE IMPLEMENTATION that has the results.  The Implementation can make catalog refs from the json, supplying a query (because it has _catalog and knows its own origin) and an entity type (because it was specified) and you wind up with a list of properly typed, anchored entity refs.  

what about reference entities? same again-- the external refs are supplied-- so you create flow refs (again, using the query) and create ref exchanges and install them

But then as soon as you want to do something with flows, say print them, you will need *their* reference entities-- but there are probably only like 20 of those-- and you can get them on an as-needed basis... or better... get local quantities from the Qdb.
NOW we are getting into some serious higher-order shits-fucked-up.  Because we just took out the provision that merged quantities from differing sources, for the express reason that we wanted different versions of LCIA methods to track differently.  (Looks like we can fix that just by asking the quantity if it is an lcia method, and then deciding to merge based on that) (DONE)

OOOOOOhKAAAAY.  anyway, we DO need to retrieve the quantities from remote, on an as-needed basis, and then subsequently add them to the Qdb (with merge in the case of non-LCIA-methods) for unit conversions to operate smoothly.  
The only way this is a problem is if iwe try to print a large set of flows, based on their catalog refs, but none of them have compartment information retrieved... at present that will generate a large number of separate get_property('Compartment') requests, which is terrible.

We don't want flow refs.  we want actual flows.

No, we want refs.  There is no way around it, the flows have to be queried from the remote source. The only advantage would be to querying them all at once- but again, you have a massive size issue.  Ecoinvent 3.2 undefined has 4680 flows totalling 2,877,168 bytes unzipped (253,306 bytes gzipped)

we do NOT want to ever run into the situation of having 1,800 distinct get_property() queries sent to a remote server.  

I think the Antelope server needs to send back a complete flow signature with every request that involves flows (minus characterizations) so that at least reference entities and compartments come through.

This means that exchanges and characterizations need to serialize differently for antelope requests. no prob.

then flow refs need to know how to query their own characterizations from remote.  and the antelope server needs to pass quantity requests through the Qdb to find the local match.

ok, this is helpful.

Back to our narrative.

 * You have a query, you make a request-- say for ALL PROCESSES having Name='steel'.
 * the Query gets an implementation-- the implementation is simply a passthrough to a web request, which spits back JSON
   = the JSON is a list of processes, with only reference exchanges, with the full (uncharacterized) flows
 * the implementation takes the passthrough stuff and makes it into a list of anchored, fully populated CatalogRefs
 * the implementation gives these back to the catalog to cache them
 * the implementation returns them 


Fri 2017-09-22 15:08:00 -0700

It is really clarifying (and agonizing) to think this through in terms of what functionality we actually want on a thin client.  e.g. charts? (if so, we need matplotib) background computation? (scipy) and so on.



It's really all about the INTERFACE (as specified in iquery.py).  This SHOULD tell us everything we would ever want to know (need to add: characterizations for flows; terminations for fragments (?) )
NO: terminations are not included.  you get traversal results for fragments.  you do need to know supported scenarios. 

Let's audit the use of _archive in the Implementations-- and then call it a day.

60 usages.  Most of them are retrieve_or_fetch_entity-- so the question is when does the retrieved entity need to be the literal entity and when can it be a remote ref?


Basic: 9 usages
 * __getitem__: return self._archive[item] --> not valid for remote
#* __str__: needs _archive.source --> should be known from resource
 * characterize(): abandoning
#* fetch: -> retrieve_or_fetch_entity -> catalog ref OK!
#* get: -> same as fetch
#* get_item: -> catalog ref OK!
#* get_reference: catalog ref OK!
#* get_uuid: -> catalog ref OK!
#* origin: _archive.ref --> should be known from resource

Index: 7 usages
#* .flows(**kwarg) --> need implementation
#* .processes(**kwarg) ""
#* .quantities(**kwarg) ""
 * .get -> useless; already in basic
#* .terminate() --> need implementation
#* .originate() ""
 * .mix --> jury's still out

Inventory: 6 usages

Short version is, no, this is going to need to be completely rewritten (and background too)
maybe quantity provides the means how: simply allow the _archive to override the implementation.
I think that's the winning approach.
and then the implementation provides the default mechanism for local archives.

The last question, then, is whether we want to make the Catalog the default cache for ALL archives and not just remote ones? i.e. change BasicImplementation.  I think not.

---crufty---

Background: this is the hardest one. do it last.
 * if static (web version is no) then we install a local background manager
   since it's not static, we install the proxy-- 
   but it's not local- so we don't want a fg proxy, we want a web proxy that will actually pass
	    


The implementation hands those BACK to the 

After that, ALL the non-metadata queries should come down to implementation. Then the IMPLEMENTATION is either:

 * (for REMOTE providers) ask the antelope server and take the result.  
   RO: Strings-- nothing to cache, nothing to do. Just give it to the user. (only for Quantity iface)
   R1: entities-- just straight json, except use it to make a CatalogRef




==========
Tue Sep 19 15:08:01 -0700 2017

Progress?  We made the decision to refactor ForegroundCatalog but we have yet to actually do it.

We DO have a test case, however: to successfully import the AUOMA model.  So let's start there.

No, let's first start by refactoring foreground_catalog, and then using AUOMA as a test case from there.

first mistake of the day: should have merged WaterfallCharts into master instead of into ReCatalog.

oh well live and learn.  ReCatalog will be merged soon enough.

So what do we want to accomplish with Foreground Catalog?

 * if fragments are just another entity type, why do we need another catalog type? ans: we don't.
   - previously, we utilized the difference when de-serializing FlowTerminations.  If they terminate to processes, we created a catalog_ref; otherwise, we just retrieved the fragment itself.
   - but now, we want that catalog ref to stand in for fragment, just as it stands in for process.
   - all we need to do is provide foregrounds as a new LcResource.
     = we can do that with existing code using catalog.new_resource()
     = only we can't EXACTLY, because an LcForeground needs a catalog as an argument.  We can provide it- but we should do so automatically.
     == Ah! catalog is already provided to create_archive() because it's needed in archive_from_json-- so we just need to add another case (done)
   - so wtf is the ForegroundCatalog supposed to do?? we identified 4 features:
     ** ed: hi, I'm ed
     ** __getitem__(self, name) -> retrieves named foreground.  We DID want foregrounds to be scoped distinctly.
     ** frag()
     ** fragment_lcia()

The editor is [only, presently] used in the LcForeground. So why don't we put the fucking editor straight in the fucking foreground?
 1. because the editor needs a Qdb
 2. because we only want to have one editor. (??)

So why don't we just put the fucking editor in the LcCatalog?
 3. because we want to disallow fragment creation for simple catalogs? seems like an odd / quixotic decision.
 4. because LcCatalog is already too heavy? just like catalog_ref?

Getitem could be useful for catalogs in general-- presently it pulls the archive by name.  could just transplant that.

Frag() -- aha, now this is tricky.  We need a way of spooling through the existing, LOADED foregrounds and looking for a fragment by ref or leading uuid portion.-  That's as simple as filtering by type

Fragment LCIA. This gets to a more fundamental question.
Fragment LCIA operates on a list of FragmentFlows (and relies on flow terminations caching nested fragment flows internally)
 - it needs qdb
 - it needs to tell qdb to load factors
 - it needs to call itself recursively.

So... how are we going to be invoking this in the REAL world?  if we are going to be calling it as a catalog_ref method, then we want to model the behavior of the entity itself.
EXCEPT-- a fragment ENTITY does not know how to compute its own LCIA-- it needs a catalog for that.
Is there a precedent? YES- catalog_refs have all sorts of methods (mainly background methods) that don't work on literal entities.

So- IF we made Fragment LCIA a catalog_ref function-- then what would it do, and where would it be implemented?
 ans: it would be implemented in the inventory interface... the InventoryImplementation inherits the BasicImplementation, which keeps a pointer to the catalog, which has a Qdb
 Better- the BasicImplementation is also one of n places where we're already keeping a log of characterized quantities-- but that's a different list-- because that's a list of quantities that are looked up externally and applied to native flows in the archive
 so hmm.

NO NO NO. The Qdb ALONE should keep track of what quantities have been loaded into the Qdb.  Right now that is not done because the Qdb only has add_cf, not add_all_cfs_for_quantity.

I'm seriously recursing here....

where are we at?

AUOMA Testing
-> ForegroundCatalog refactor
  -> fragment_lcia()
    -> catalog_ref()
      -> Catalog.load_lcia_factors()
        -> Qdb.get_canonical_quantity()

What on earth do we want a canonical quantity for?  ans: the canonical quantity is the actual entity stored in the Qdb.  We want to retrieve it if we want to know whether it's loaded?  BUT- the literal (canonical) quantity ITSELF can't tell me its factors, because that is a catalog function.

Where is get_canonical_quantity used? 6 places
 * LcCatalog.lcia() -> to test if loaded; as argument to do_lcia
 * LcCatalog.load_lcia_factors() -> to store a list of loaded quantities
 * Qdb.do_lcia() -> to dance around a bit before retrieving the same quantity with get_quantity() (which is functionally identical)
 * in BackgroundImplementation.bg_lcia(), in an if False: block (linked with BasicImplementation.characterize()
 * in QuantityImplementation.lcia_methods(), in an apparent NOP (return value not used)
 * in QuantityImplementation.quantities(), in the same apparent NOP (return value not used)
   = now it's not really a NOP because it calls _get_q_ind, which ADDS the quantity if it doesn't already exist. But isn't there a better way of doing that?

More to the point- the original question still stands: Why do we ever WANT to obtain a "canonical" quantity?  What does it allow us to do that having a quantity_ref.uuid doesn't?



==========
Tue Aug 01 13:25:08 -0700 2017

This is still stalled out.

We're having a breakdown of design discipline.

Tue 2017-08-01 15:14:48 -0700

Big improvement (not yet tested though): re-designed QueryInterface to inherit from a whole bunch of abstract base classes that each only specifies how different resource types should behave. Then instead of re-inheriting from the QueryInterface, I now have a separate set of implementation classes that actually implement the endpoints.

NOTHING is tested or even compiled yet.  but it already feels better.

Next tasks: get stuff out of catalog.py, and then get more stuff out of foreground_catalog.py

Tue 2017-08-01 22:59:37 -0700

OK- LcCatalog actually looks pretty good. We have:
 - a whole list of imports, many of them local

Line 55: class LcCatalog(object):
then we have 60 lines of root-path-dependent constants

line 118: __init__
     _rootdir
     _resolver
     _entities ** DO WE REALLY WANT THIS? I THINK WE SHOULD GET RID OF THIS **
     _qdb

The actual catalog:

     _archives: source to LcArchive
     _names: ref:interface to source
     _nicknames: shorthand -> source

     _lcia_methods: ** WHERE SHOULD I KEEP TRACK OF THIS? **  (already 2+ places: in catalog, in qdb, in basic)

Lines 147-230: make, get, retrieve archives and resources

Lines 245-287: names, sources, and interfaces

lines 288-381: index, cache, and archive resources

lines 382-418: dereference and retrieve entities

lines 419-447: get interfaces

lines 448-472: Utilities
	query(origin) - returns a CatalogQuery
	lookup(ref) - ref = CatalogRef - returns "the lowest-priority origin to contain the entity". sounds like it could have greater utility.
	fetch(ref) - this should also permit greater specification
	entity_type(ref) - could probably be done more efficiently

lines 473-eof (538): Qdb Interaction
	is_elementary(flow)
	is_loaded(lcia) ** THIS AGAIN **
	load_lcia_factors(ref)
	lcia(p_ref, q_ref) -- the core function of the catalog
	annotate(flow, quantity, ...) -- gets stored in local Qdb
	quantify(xxx) - not helpful
	   

Glue / misc:
line 144-146: quantities()
line 232-235: privacy(ref)
line 236-244: flowables grid

All seem fair.

(commit)

Now, turning to foreground_catalog...

WHY, exactly, are we creating an entire separate (and ad hoc) infrastructure for handling "foregrounds", which in fact correspond to physical sources in the EXACT same way that LcResources do, and we ALREADY HAVE a catalog for handling LcResources?

That is a very good question. I think basically all that stuff can COME OUT, modulo some light details regarding search order / preference / priority.

line 18-26: class ForegroundCatalog(LcCatalog): and docstring
lines 27-34: a list of foregrounds known outside the LcResolver context
lines 35-45: we want sources to exclude foregrounds for some reason? so we ARE storing foregrounds in the same dict as other sources, BUT monkeying with the machinery to except them?

line 46: __init__
     _foregrounds -> only loaded ones (so, unlike _archives how?)
     _known_fgs -> redundant to resolver
**     ed -> Hi, I'm ed.

** lines 53-65: __getitem__ this is maybe appealing
lines 66-78: this terrible name-canonicalizing. Why am I forcing names to start with 'foreground' and have no specified interface, when the existing machinery already provides ref:foreground?
ans: because I want to be able to specify a scope of all foregrounds? well, just make the resolver work with '' input

lines 79-109: load / add foregrounds
lines 110-118: known foregrounds management. again, this is a matter that LcResources already solve
lines 119-127: listing + showing foregrounds

Then we have 128-155: re-implement _dereference as _retrieve for no good reason
** lines 156-164: the first foreground-specific feature, other than __getitem__ but that is also good: get fragment by reference or leading portion of uuid

lines 165-184: Re-implemented lookup() and fetch()

** lines 185-217 fragment_lcia(fragmentflows, q_ref)

OK. so about 32 + 8 + 12 + 1 = 25% of the class is worth keeping.  The rest should be wrapped up in LcCatalog.

Wed 2017-08-02 15:44:19 -0700

So how do we go about this?

 * Do we want 'foreground' to be included in INTERFACE_TYPES?
   - are there any characteristics of the foreground interface that are not shared by any of the others?
   - are there compelling circumstances when I would NOT want to look in foreground resources?
   = does the order of inquiry matter, or is that handled with resource priorities?

INTERFACE_TYPES uses (8):
 iquery:
  (as arguments to _perform_query)
  - get_item
  - get
  - get_reference
  - get_uuid
  (as default interfaces to resolve)
  - resolve

 test_catalog:
  - initialize uslci_bg resource

 catalog:
  - lookup(), as argument to _dereference

 lc_resource:
  - to validate interfaces argument

So add_foreground should create a resource that has 'foreground', 'index', 'inventory', and 'foreground' should be in INTERFACE_TYPES.

the only thing foregrounds can do that others can't is be WRITEABLE.  So foreground interface is all POST routines.  everything else either belongs to the catalog (fragment_lcia) or .. is traverse() and can go in inventory.  there aren't any others.

also need a scenarios interface.

(the interfaces lark is quite a gas, isn't it?)

so foreground is all about creating a foreground, creating flows, setting properties, creating fragments and specifying exchange values, and terminating. and setting balance
that's it.
scenarios-> those are just query args for exch val and termination

Thus we add 'foreground' to INTERFACE_TYPES, and we create READONLY_INTERFACE_TYPES which excludes it.

Then almost all of the above cases (6 of 8) should in fact use READONLY; and only resolve() and lc_resource validation should use the full list.

 * What other changes need to be made in order to have the LcResolver handle foregrounds?

 * What queries need to be supported in the foreground query interface?
   Foreground interface is the same as inventory interface, except that it allows for the archive to contain fragments.
   = is there an affirmative characteristic of LcArchive that PREVENTS it from storing fragments? ANS: NO

Foreground audit!! Now this was recently rewritten, so I imagine it is generally useful, but let's take a look:

line 22-26: class LcForeground(LcArchive): + docstring
line 27-30: load_json_file-- this could be part of the base LcArchive; I don't see why not
line 31-46: external references mapping-- this could be part of the base LcArchive
line 47-63: __init__(path, catalog)
     Requires a catalog to deserialize fragments
     Requires that it be a foreground catalog i.e. that has an entity editor
line 64-67: editor property
line 68-77: create resolved catalog refs; also actual entities from foregrounds (DEPRECATED)
line 78-90: override add to merge instead of skipping
line 91-96: save-- write entity file, serialize fragments
line 97-167: create fragments
line 168-172: clear score caches
line 173-180: path-specific constants
line 181-240: access / retrieve fragments
line 241-245: draw tree
line 246-250: override check_counter
line 251-259: assign external references
line 260-293: load fragments
line 294-304: outmoded child_flows fn
line 305-325: save fragments
line 326-360: find linked terms and orphans.

Things to fix:
 - old child flows
 - reorganize
 - remove fragment handling from LcArchive.add(), put it into Foreground.add(). This prevents LcArchives from adding fragments.  raise (and catch?) TypeError.
 - Terminations to fragments shjould always reference the parent fragment, with the termflow used to indicate if the fragment is not being run directly.

 - figure out the upstreaming issue.  Is this just a foreground feature now? should it always and only use the catalog's native Qdb?

We need to think this over more.






==========
Tue May 02 14:53:31 -0700 2017

This is STALLED OUT.

Here's the approach:

 The CatalogRef is the magic key.  It implements all the API interfaces, and then it asks the catalog to satisfy them.
 The Catalog leans heavily on the resolver to get from a semantic ref to a physical archive.
 


==========
Tue Apr 25 15:07:52 -0700 2017

An LcCatalog provides an automated service for obtaining information about LCA data objects.

There are four different forms of (read-only) information available to be queried (see API.md):

 * Quantity data are informations about the quantitative properties of flows.  These are stored in a quantity database, which can exist independently of any inventory information.  Quantity data apply to 'flowables', often when going into or coming out of different 'compartments'.  An LcFlow is a flowable and a particular compartment.

 * Catalog data are metadata and non-quantitative data about entities, including their names, characteristics. At least a subset of these are generally publicly available for all entities.

 * Foreground data are quantitative data that describe computational information about the entities when used in models.  For flows, these include characterizations. For processes, these include exchanges.

 * Background data are quantitative data that describe the aggregated life cycle inventory or life cycle impact assessment results for processes embedded in computable databases.

When a catalog ref is looked up, the catalog specifies whether the entity is known to the catalog in a catalog, fg, or bg sense.

This means that the catalog is also separate from the foreground in a way that's different from before. it's more focused on data retrieval (as should be).

We still haven't broached the study construction- which comes next.

Thu 2017-04-27 12:17:52 -0700

But-- for the time being, the important facts are: the catalog PROVIDES access to the API routes specified in API.md.  The Catalog CAN be implemented as a web service.  The catalog CAN ALSO operate as a web CLIENT- in which case the data source would be another instance of the catalog service-- but this is a complicated step that requires the catalog to de-serialize JSON data.  That can get folded into the archive factory, though, I think.

SO, TO SUMMARIZE:
 - the LcCatalog is the culmination of the semantic catalog work. the idea is to translate a REST-style query into usable lca data. I *think* the API spec is adequate to do complete LCA. (minus uncertainty; minus study-specific allocation).

 - The catalog operates on a catalog ref, which is an origin + external ref, which should be unique.

 - the STUDY should use the catalog to obtain data and also enable the user to directly input data. Everything is fragments; nothing is processes.

 - fragments are basically exchanges, and terminations are complementary exchanges.  Thus a terminated fragment is a link.

REMAINING WORK:

 - implement the catalog, obviously

 - implement the CatalogInterface, ForegroundInterface, BackgroundInterface.  get them tested and running.  Those interfaces are eminently testable.

 - re-design the foreground to be the study-- construct fragments incrementally using the API queries.

 - for now we don't need fg lcia, since all the child flows will be rendered as fragments.  If we want, we can do something where 'flows not present as child fragments but present in the process inventory can be characterized'.  but rather than jump to that, we should AUTOMATICALLY CREATE SUB-FRAGMENTS from processes, store the process ref, and then write a routine to update fragment exchange values from queries.  best of all worlds.

 - lastly, write the web service, and get it running on the lca-tools-datafiles data.  THAT will be instantly disruptive.

time to get the bus.

PLUMBING

OK, in the interest of postponing ACTUAL PROGRAMMING for as long as possible, let's continue to think through exactly how this will work.

Fri 2017-04-28 18:23:14 -0700

OK.  SOME modest progress.




Option 1: Prototype the CatalogInterface, ForegroundInterface, and BackgroundInterface.

 * CatalogInterface inherits from NsUuidArchive
 * key is '/'.join(origin, external_ref)
 * Catalog's entity list acts as a cache -- stores non-quantitative data for all entities requested (quant too?)
 * CatalogInterface implements the API
 * CatalogInterface composed with a Qdb (separate interface)
 * CatalogInterface maintains a collection of LcArchives

 * CatalogInterface has a resolver input file that maps semantic roots to lists of data sources
   - each data source has attributes:
     = Priority - lowest priority accessed first
     = dataSourceType - argument to archive_factory
     = dataContents - controlled vocabulary: 'catalog', 'foreground', 'background'
       ** foreground provides catalog
       ** foreground archive input to BackgroundManager provides background
       ** background provides catalog
       ** background cannot provide foreground
     = ExcludeRoutes: list of API routes that the source should not provide
     = AccessControl: controlled vocabulary:
       ** 'open' -- any query is answered
       ** 'metered' -- authentication must be provided; every query is billable
       ** 'protected' -- authentication must be provided; authorization TBD
       ** ... others TBD

There is going to have to be some kind of regression.  When a query is received, the resolver needs to:
 - receive the query, including origin + ref + authentication info
 - determine whether the catalog can answer the query at all
   = determine if a source is known for the given origin
   = determine if the request is authorized to access the source
     ** list the same source multiple times?? sure-- the thing that gets 
   = determine if the query type (catalog, fg, bg) is available
 - determin
